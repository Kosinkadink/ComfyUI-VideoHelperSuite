import torch
from torch import Tensor
import itertools
from typing import List, Dict, Any, Tuple, Optional, Union

# å¸§æå–å·¥å…·
class ExtractFrames:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "images": ("IMAGE",),
            }
        }
    
    CATEGORY = "Video Helper Suite ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢/image"
    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("first_frame", "last_frame")
    FUNCTION = "extract_frames"

    def extract_frames(self, images: Tensor):
        """
        ä»å›¾åƒåºåˆ—ä¸­æå–é¦–å¸§å’Œå°¾å¸§
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, height, width, channels]
            
        Returns:
            first_frame: é¦–å¸§å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º [1, height, width, channels]
            last_frame: å°¾å¸§å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º [1, height, width, channels]
        """
        if images.size(0) == 0:
            raise ValueError("è¾“å…¥å›¾åƒåºåˆ—ä¸ºç©º")
        
        # æå–é¦–å¸§å’Œå°¾å¸§
        first_frame = images[0:1]  # ä¿æŒç»´åº¦ä¸º [1, h, w, c]
        last_frame = images[-1:] if images.size(0) > 1 else first_frame
        
        return (first_frame, last_frame)


# å¸§åˆå¹¶å·¥å…·
class MergeFrames:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "frames_1": ("IMAGE",),
            },
            "optional": {
                "frames_2": ("IMAGE",),
                "frames_3": ("IMAGE",),
                "frames_4": ("IMAGE",),
                "frames_5": ("IMAGE",),
            }
        }
    
    CATEGORY = "Video Helper Suite ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢/image"
    RETURN_TYPES = ("IMAGE", "INT")
    RETURN_NAMES = ("merged_frames", "frame_count")
    FUNCTION = "merge_frames"

    def merge_frames(self, frames_1: Tensor, 
                    frames_2: Optional[Tensor] = None,
                    frames_3: Optional[Tensor] = None, 
                    frames_4: Optional[Tensor] = None,
                    frames_5: Optional[Tensor] = None):
        """
        æŒ‰é¡ºåºåˆå¹¶å¤šç»„å›¾åƒåºåˆ—
        
        Args:
            frames_1: ç¬¬ä¸€ç»„å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size_1, height, width, channels]
            frames_2 åˆ° frames_5: å¯é€‰çš„é¢å¤–å›¾åƒå¼ é‡ç»„
            
        Returns:
            merged_frames: åˆå¹¶åçš„å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º [total_batch_size, height, width, channels]
            frame_count: åˆå¹¶åçš„æ€»å¸§æ•°
        """
        # æ”¶é›†æ‰€æœ‰éç©ºè¾“å…¥å¸§
        all_frames = [frames_1]
        if frames_2 is not None:
            all_frames.append(frames_2)
        if frames_3 is not None:
            all_frames.append(frames_3)
        if frames_4 is not None:
            all_frames.append(frames_4)
        if frames_5 is not None:
            all_frames.append(frames_5)
        
        # æ£€æŸ¥æ‰€æœ‰å¸§çš„å½¢çŠ¶å…¼å®¹æ€§ (é«˜åº¦å’Œå®½åº¦)
        if not all(frames.shape[1:] == frames_1.shape[1:] for frames in all_frames):
            # è·å–ä¸åŒ¹é…çš„å½¢çŠ¶ä¿¡æ¯ï¼Œç”¨äºé”™è¯¯æ¶ˆæ¯
            shapes = [frames.shape for frames in all_frames]
            raise ValueError(f"æ‰€æœ‰è¾“å…¥å¸§çš„å°ºå¯¸å¿…é¡»ç›¸åŒï¼Œä½†æ”¶åˆ°äº†ä»¥ä¸‹å½¢çŠ¶ï¼š{shapes}")
        
        # æŒ‰é¡ºåºåˆå¹¶æ‰€æœ‰å¸§
        merged_frames = torch.cat(all_frames, dim=0)
        frame_count = merged_frames.size(0)
        
        return (merged_frames, frame_count)

# è¦åœ¨ nodes.py ä¸­å¯¼å…¥çš„èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "VHS_ExtractFrames": ExtractFrames,
    "VHS_MergeFrames": MergeFrames,
}

# è¦åœ¨ nodes.py ä¸­å¯¼å…¥çš„èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "VHS_ExtractFrames": "Extract Frames (First/Last) ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢",
    "VHS_MergeFrames": "Merge Frames Sequence ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢",
}
